# LocalLLM

**Author**: Gerardo Solis

## Overview

- The goal of this project is to use Large Language Models (LLMs) on consumer-grade hardware.

- The importance of this project lies in the ability to run LLMs locally using Ollama, which ensures security and privacy.

- Specifically, models like DeepSeek and Qwen are utilized because they allow for more granular control over the operations performed.

---

## Visual Overview

- ![LocalLLM Application Design](./assets/LocalLLM%20Application%20Design.png)

---

## Motivation

- This project aims to provide a local environment where you can continuously update the model and perform tasks specific to red team operations without restrictions influenced by guardrails set by platforms.

---

## Inspiration

- The inspiration for this project came from the need to create a Retrieval-Augmented Generation (RAG) system tailored to red team operations.

- Since questions or projects related to red teaming are often censored, having a local LLM allows for more flexible and unrestricted research and testing.

---

## Skills Development

- I am entirely self-taught in this subject, so there may be inaccuracies or areas that need improvement. 

- This project serves as an opportunity to enhance my full-stack development skills.
